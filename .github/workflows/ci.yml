name: CI/CD

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:  # Allow manual deployment

env:
  GO_VERSION: '1.23'
  AWS_REGION: us-east-1

jobs:
  test:
    name: Test
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: nimbus
          POSTGRES_PASSWORD: nimbus123
          POSTGRES_DB: nimbus
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Download dependencies
        run: go mod download

      - name: Run tests
        run: go test -v -race -coverprofile=coverage.out ./...
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_USER: nimbus
          DB_PASSWORD: nimbus123
          DB_NAME: nimbus
          REDIS_HOST: localhost
          REDIS_PORT: 6379

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.out
          fail_ci_if_error: false

  build:
    name: Build
    runs-on: ubuntu-latest
    needs: [test]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Build binary
        run: CGO_ENABLED=0 GOOS=linux go build -o nimbus-gateway ./cmd/gateway

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: nimbus-gateway
          path: nimbus-gateway

  docker:
    name: Docker Build & Push
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    permissions:
      id-token: write
      contents: read

    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: |
            ${{ steps.login-ecr.outputs.registry }}/nimbus-prod:${{ github.sha }}
            ${{ steps.login-ecr.outputs.registry }}/nimbus-prod:latest
          cache-from: type=registry,ref=${{ steps.login-ecr.outputs.registry }}/nimbus-prod:buildcache
          cache-to: type=registry,ref=${{ steps.login-ecr.outputs.registry }}/nimbus-prod:buildcache,mode=max

  deploy:
    name: Deploy to ECS
    runs-on: ubuntu-latest
    needs: [migrate]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: production

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Deploy to ECS
        run: |
          if aws ecs describe-services --cluster nimbus-prod --services nimbus-prod --query "services[0].serviceName" --output text 2>/dev/null | grep -q "nimbus-prod"; then
            echo "✅ Updating ECS service..."
            aws ecs update-service \
              --cluster nimbus-prod \
              --service nimbus-prod \
              --force-new-deployment
            echo "✅ Deployment triggered"
          else
            echo "⚠️  ECS service not found - skipping deployment (infrastructure needs to be created first)"
            echo "To deploy, first create ECS infrastructure using Terraform or AWS Console"
          fi

  migrate:
    name: Run DB migrations
    runs-on: ubuntu-latest
    needs: [docker]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    permissions:
      id-token: write
      contents: read

    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr-migrator
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push migrator image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr-migrator.outputs.registry }}
          ECR_REPOSITORY: nimbus-prod
          IMAGE_TAG: migrator-${{ github.sha }}
        run: |
          docker build -f Dockerfile.migrator -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:migrator-latest
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:migrator-latest

      - name: Check if ECS infrastructure exists
        id: check-ecs
        run: |
          if aws ecs describe-clusters --clusters nimbus-prod --query "clusters[0].clusterName" --output text 2>/dev/null | grep -q "nimbus-prod"; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "✅ ECS cluster exists"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "⚠️  ECS cluster not found - skipping migrations"
          fi

      - name: Run migrations via one-off ECS task
        if: steps.check-ecs.outputs.exists == 'true'
        env:
          AWS_REGION: ${{ env.AWS_REGION }}
          CLUSTER: nimbus-prod
          SERVICE: nimbus-prod
          DB_USER: nimbus
          DB_NAME: nimbus
          LOG_GROUP: /ecs/nimbus-prod
          ECR_REGISTRY: ${{ steps.login-ecr-migrator.outputs.registry }}
          ECR_REPOSITORY: nimbus-prod
          IMAGE_TAG: migrator-${{ github.sha }}
        run: |
          set -euo pipefail

          SERVICE_DESC=$(aws ecs describe-services --cluster "$CLUSTER" --services "$SERVICE" 2>/dev/null || echo '{"services":[]}')
          
          # Check if service exists
          if [ "$(echo "$SERVICE_DESC" | jq -r '.services | length')" -eq 0 ]; then
            echo "⚠️  ECS service not found - skipping migrations (infrastructure needs to be created first)"
            exit 0
          fi

          TASK_DEF_ARN=$(echo "$SERVICE_DESC" | jq -r '.services[0].taskDefinition')
          NETWORK_CFG=$(echo "$SERVICE_DESC" | jq -r '.services[0].networkConfiguration.awsvpcConfiguration')
          SUBNETS=$(echo "$NETWORK_CFG" | jq -r '.subnets | join(",")')
          SECURITY_GROUPS=$(echo "$NETWORK_CFG" | jq -r '.securityGroups | join(",")')

          TASK_DEF=$(aws ecs describe-task-definition --task-definition "$TASK_DEF_ARN")
          EXEC_ROLE=$(echo "$TASK_DEF" | jq -r '.taskDefinition.executionRoleArn')
          TASK_ROLE=$(echo "$TASK_DEF" | jq -r '.taskDefinition.taskRoleArn')
          CPU=$(echo "$TASK_DEF" | jq -r '.taskDefinition.cpu')
          MEMORY=$(echo "$TASK_DEF" | jq -r '.taskDefinition.memory')

          DB_HOST=$(aws rds describe-db-instances --db-instance-identifier nimbus-prod --query "DBInstances[0].Endpoint.Address" --output text)
          DB_PASSWORD=$(aws secretsmanager get-secret-value --secret-id nimbus-prod-db-password --query SecretString --output text)
          DATABASE_URL="postgres://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:5432/${DB_NAME}?sslmode=require"

          cat > /tmp/migrator-td.json <<EOF
          {
            "family": "nimbus-migrator",
            "networkMode": "awsvpc",
            "requiresCompatibilities": ["FARGATE"],
            "cpu": "${CPU}",
            "memory": "${MEMORY}",
            "executionRoleArn": "${EXEC_ROLE}",
            "taskRoleArn": "${TASK_ROLE}",
            "containerDefinitions": [
              {
                "name": "migrator",
                "image": "${ECR_REGISTRY}/${ECR_REPOSITORY}:${IMAGE_TAG}",
                "essential": true,
                "environment": [
                  { "name": "DATABASE_URL", "value": "${DATABASE_URL}" }
                ],
                "logConfiguration": {
                  "logDriver": "awslogs",
                  "options": {
                    "awslogs-group": "${LOG_GROUP}",
                    "awslogs-region": "${AWS_REGION}",
                    "awslogs-stream-prefix": "migrator"
                  }
                }
              }
            ]
          }
          EOF

          TASK_DEF_MIGRATOR=$(aws ecs register-task-definition --cli-input-json file:///tmp/migrator-td.json --query 'taskDefinition.taskDefinitionArn' --output text)

          RUN_OUTPUT=$(aws ecs run-task \
            --cluster "$CLUSTER" \
            --launch-type FARGATE \
            --task-definition "$TASK_DEF_MIGRATOR" \
            --network-configuration "awsvpcConfiguration={subnets=[${SUBNETS}],securityGroups=[${SECURITY_GROUPS}],assignPublicIp=DISABLED}")

          TASK_ARN=$(echo "$RUN_OUTPUT" | jq -r '.tasks[0].taskArn')
          if [ "$TASK_ARN" = "null" ] || [ -z "$TASK_ARN" ]; then
            echo "Failed to start migration task" >&2
            echo "$RUN_OUTPUT" >&2
            exit 1
          fi

          aws ecs wait tasks-stopped --cluster "$CLUSTER" --tasks "$TASK_ARN"

          EXIT_CODE=$(aws ecs describe-tasks --cluster "$CLUSTER" --tasks "$TASK_ARN" | jq -r '.tasks[0].containers[0].exitCode')
          if [ "$EXIT_CODE" != "0" ]; then
            echo "Migration task failed with exit code $EXIT_CODE" >&2
            LOG_STREAM=$(aws ecs describe-tasks --cluster "$CLUSTER" --tasks "$TASK_ARN" | jq -r '.tasks[0].containers[0].logConfiguration.options["awslogs-stream"]')
            echo "Check CloudWatch logs for details (group ${LOG_GROUP}, stream prefix migrator)" >&2
            exit 1
          fi

          echo "Migrations completed successfully"
